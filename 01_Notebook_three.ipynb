{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7XKw6Hce0Bn3vXwRcNeG8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatthewHawksbyGithub/pytorch-deep-learning/blob/main/01_Notebook_three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "id": "k5WXUViPYzm1",
        "outputId": "1122958f-a452-4ffa-aa00-33aee90fea53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])\n",
            "Epoch: 0 | Loss: 0.6161779165267944 | Test loss: 0.584976315498352\n",
            "Epoch: 10 | Loss: 0.5009680986404419 | Test loss: 0.45026636123657227\n",
            "Epoch: 20 | Loss: 0.3857582211494446 | Test loss: 0.3155565857887268\n",
            "Epoch: 30 | Loss: 0.2705483138561249 | Test loss: 0.18084672093391418\n",
            "Epoch: 40 | Loss: 0.15533843636512756 | Test loss: 0.04613689333200455\n",
            "Epoch: 50 | Loss: 0.05875825136899948 | Test loss: 0.06886560469865799\n",
            "Epoch: 60 | Loss: 0.04580377787351608 | Test loss: 0.0947304219007492\n",
            "Epoch: 70 | Loss: 0.04181947931647301 | Test loss: 0.09405827522277832\n",
            "Epoch: 80 | Loss: 0.03831038624048233 | Test loss: 0.08853326737880707\n",
            "Epoch: 90 | Loss: 0.034879542887210846 | Test loss: 0.08094760030508041\n",
            "Epoch: 100 | Loss: 0.03144557401537895 | Test loss: 0.07267506420612335\n",
            "Epoch: 110 | Loss: 0.028006773442029953 | Test loss: 0.0644025206565857\n",
            "Epoch: 120 | Loss: 0.024577608332037926 | Test loss: 0.056816864758729935\n",
            "Epoch: 130 | Loss: 0.021141955628991127 | Test loss: 0.04854437708854675\n",
            "Epoch: 140 | Loss: 0.017704108729958534 | Test loss: 0.04095881059765816\n",
            "Epoch: 150 | Loss: 0.014275660738348961 | Test loss: 0.03268631175160408\n",
            "Epoch: 160 | Loss: 0.010838326066732407 | Test loss: 0.02441384270787239\n",
            "Epoch: 170 | Loss: 0.007402160204946995 | Test loss: 0.016828250139951706\n",
            "Epoch: 180 | Loss: 0.0039735049940645695 | Test loss: 0.008555757813155651\n",
            "Epoch: 190 | Loss: 0.008990185335278511 | Test loss: 0.005145937204360962\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-ebf98326848a>\u001b[0m in \u001b[0;36m<cell line: 367>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m \u001b[0mplot_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-ebf98326848a>\u001b[0m in \u001b[0;36mplot_predictions\u001b[0;34m(train_data, train_labels, test_data, test_labels, predictions)\u001b[0m\n\u001b[1;32m     54\u001b[0m                      test_labels=y_test,predictions=None):\n\u001b[1;32m     55\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training Data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"g\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Testing Data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/_api/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m    227\u001b[0m             f\"module {cls.__module__!r} has no attribute {name!r}\")\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'scatter'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn #nn contains all of PyTorch's neural networks components.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#Acquire data.\n",
        "#Convert into tensors.\n",
        "#Build a model, or pick a pretrained model.\n",
        "#Fit the model to the data and make a prediction.\n",
        "#evaluate the model.\n",
        "#Improving through experimentation.\n",
        "#Save and reload your trained model.\n",
        "\n",
        "#Exploring an example PyTorch end-to-end workflow.\n",
        "\n",
        "#Let's recreate some known data using a linear regression formula.\n",
        "#Making a straight line with known parameters. y = a +bx b\n",
        "# is the weight and a is the bias.\n",
        "\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "\n",
        "#create\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "X = torch.arange(start,end,step).unsqueeze(dim=1)\n",
        "y = weight * X + bias\n",
        "\n",
        "X[:10], y[:10], len(X), len(y)\n",
        "\n",
        "### Splitting data into training and test sets\n",
        "#Generalization is the ability for a machine learning model to perform\n",
        "#well on data that it hasn't seen before.\n",
        "\n",
        "#Training set, validation set and testing set.\n",
        "#Training is 60-80\n",
        "#Validation is 10-20, test set is 10-20.\n",
        "\n",
        "#80 20 is the most common split.\n",
        "\n",
        "train_split = int(0.8 * len(X))\n",
        "x_train, y_train = X[:train_split], y[:train_split]\n",
        "x_test, y_test = X[train_split:], y[train_split:]\n",
        "\n",
        "#scikit learn has a train test split that includes randomness.\n",
        "\n",
        "len(x_train), len(y_train), len(x_test), len(y_test)\n",
        "\n",
        "#Let's visualize the data with matplotlib:\n",
        "\n",
        "def plot_predictions(train_data=x_train,train_labels=y_train,test_data=x_test,\n",
        "                     test_labels=y_test,predictions=None):\n",
        "  plt.figure.Figure(figsize=(10,7))\n",
        "  plt.scatter(train_data,train_labels,c=\"b\",s=4,label=\"Training Data\")\n",
        "  plt.scatter(test_data,test_labels,c=\"g\",s=4,label=\"Testing Data\")\n",
        "  if predictions is not None:\n",
        "    plt.scatter(test_data,predictions,c=\"r\",s=4,label=\"Predictions\")\n",
        "  plt.legend(prop={\"size\":14});\n",
        "\n",
        "#plot_predictions()\n",
        "\n",
        "#First PyTorch Model:\n",
        "#Using a linear regression model.\n",
        "#Create a linear regression model class.\n",
        "\n",
        "class LinearRegressionModel (nn.Module):\n",
        "  #<-- Almost everything inherits from nn.Module.\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.weights = nn.Parameter(torch.randn(1,\n",
        "                                            requires_grad=True, #\n",
        "                                            dtype=torch.float))\n",
        "    self.bias = nn.Parameter(torch.randn(1,\n",
        "                                         requires_grad=True,\n",
        "                                         dtype=torch.float))\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor: #X is the input data.\n",
        "      return self.weights * x + self.bias #The goal of this model is that\n",
        "      #the model will start with random values for the weights and biases\n",
        "      #and iteratively approach the known values for the weights and biases.\n",
        "      #An algorithm called 'gradient descent'.\n",
        "\n",
        "  #Two main algorithms: Gradient descent and backpropagation.\n",
        "\n",
        "  #requires_Grad -> These algorithms are already implemented\n",
        "  #torch.autograd\n",
        "\n",
        "  #FORWARD DEFINES THE COMPUTATION MADE AT EVERY CALL!\n",
        "  #IF YOU'RE GOING TO SUBCLASS nn.Module, you MUST HAVE\n",
        "  #A FORWARD METHOD.\n",
        "\n",
        "  ###PyTorch Model building essentials;\n",
        "\n",
        "  #torch.nn - contains all the building blocks for computational graphs\n",
        "\n",
        "  #torch.nn.Parameter - What parameters our model will try and learn.\n",
        "\n",
        "  #torch.nn.Module - the base class for all neural network modules.\n",
        "\n",
        "  #torch.optim - optimizers are here.\n",
        "  #torchvision.models <=premade models for computer vision\n",
        "\n",
        "  #torchmetrics\n",
        "  #torch.utils.tensorboard\n",
        "\n",
        "  #Create a random seed:\n",
        "  torch.manual_seed(42)\n",
        "\n",
        "  #Create an instance of the model.\n",
        "model_0 = LinearRegressionModel()\n",
        "print(model_0.state_dict())\n",
        "\n",
        "#weights and bias are just random unaltered numbers right now.\n",
        "#Predictive power is nil.\n",
        "\n",
        "with torch.inference_mode():#Inference mode turns off gradient training.\n",
        "  y_pred = model_0(x_test) #predictions are faster without inference mode.\n",
        "\n",
        "#y_pred\n",
        "#plot_predictions(predictions = y_pred)\n",
        "\n",
        "## The whole idea of train is for a model to move some 'unknown' parameters\n",
        "#(These may be random) to some 'known' parameters.\n",
        "\n",
        "#Or from a poor-representation of the data, to a better representation of the data.\n",
        "\n",
        "#How to measure how good or poor the representation is of the data?\n",
        "#Use a loss function. \"Minimizing the loss function\"\n",
        "#PyTorch has many loss functions built in.\n",
        "#a loss function may also be called cost function or criterion.\n",
        "\n",
        "#Mean absolute error.....\n",
        "#Optimizer -> takes into account the loss of the model and adjusts the model's\n",
        "#parameters(e.g. weights and biases)\n",
        "\n",
        "#Setting up a loss function and optimizer in PyTorch\n",
        "\n",
        "#for PyTorch we need a training loop and a testing loop.\n",
        "\n",
        "#L1Loss\n",
        "#SETTING UP A LOSS FUNCTION:\n",
        "\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "#Setup an optimizer i.e. SGD Stochastic Gradient Descent\n",
        "#Tries random numbers repeatedly to minimize the loss function.\n",
        "\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
        "                            lr = 0.01)\n",
        "\n",
        "\n",
        "#A hyperparameter is a value that you would set yourself.\n",
        "#Learning rate here affects the 'step' of the optimizer; smaller learning rates\n",
        "#give a smaller step rate. Parameter will change faster with a higher lr.\n",
        "\n",
        "### Building a training loop in PyTorch\n",
        "\n",
        "#A couple of things we need in a training loop:\n",
        "#0. Loop through the data.\n",
        "#1. Forward Pass, uses the Forward() function in the model. \"forward propagation\"\n",
        "#2. Calculate the loss. (compare forward pass predictions to ground truth labels)\n",
        "#3. Optimizer zero-grad\n",
        "#4. Loss Backwards - move backwards through the network to calculate the gradients\n",
        "#of the parameters of our model with respect to the loss (BACKPROPAGATION)\n",
        "#5. Optimizer step - use the optimizer to adjust our model's parameters to\n",
        "#try and improve the loss. (GRADIENT DESCENT)\n",
        "#WHAT IS GRADIENT? GRADIENT IS A SLOPE.\n",
        "\n",
        "#A epoch is one loop through the data.\n",
        "epochs = 100\n",
        "\n",
        "epoch_count = []\n",
        "loss_values = []\n",
        "test_loss_values = []\n",
        "\n",
        "#0.\n",
        "for epoch in range(epochs): #epochs are a hyperparameter.\n",
        "  model_0.train() #Must set the model to training mode. The other is evaluation mode.\n",
        "  #1. Forward pass\n",
        "  y_pred = model_0(x_train)\n",
        "  #2. Recalculate the loss.\n",
        "  loss = loss_fn(y_pred, y_train)\n",
        "  #3. optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "  #Backpropagation\n",
        "  loss.backward()\n",
        "  #Step the optimizer\n",
        "  optimizer.step() #by default, how the optimizer changes will\n",
        "  #accumulate through the loop. So the changes must be zeroed during\n",
        "  #each loop iteration.\n",
        "  model_0.eval()\n",
        "  with torch.inference_mode():\n",
        "    test_pred = model_0(x_test)\n",
        "    test_loss = loss_fn(test_pred, y_test)\n",
        "    if (epoch%10 == 0):\n",
        "      epoch_count.append(epoch)\n",
        "      loss_values.append(np.array(loss))\n",
        "      test_loss_values.append(np.array(test_loss))\n",
        "      #print(f\"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}\")\n",
        "\n",
        "#x test should predict y test.\n",
        "\n",
        "#plot the loss curves\n",
        "\n",
        "#plt.plot(epoch_count, loss_values, label = \"Train loss\")\n",
        "#plt.plot(epoch_count, test_loss_values, label= \"Test loss\")\n",
        "#plt.title(\"Training and test loss curves\")\n",
        " #Can be automated later on.\n",
        " #Reducing loss\n",
        "\n",
        "#Three methods of saving and loading a pytorch object;\n",
        "#torch.save() uses Python's 'pickle' format.\n",
        "#torch.load() loads a saved PyTorch object\n",
        "#torch.nn.Module.load_state_dict() loads the model's saved state dictionary.\n",
        "\n",
        "#state dict is likely less useful to look at\n",
        "# when you have millions of parameters.\n",
        "\n",
        "#The optimizer also has a state dict.\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "#Can save from google colab to google drive.\n",
        "\n",
        "MODEL_PATH = Path(\"Models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "MODEL_SAVE_PATH\n",
        "\n",
        "#print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_0.state_dict(), f=MODEL_SAVE_PATH)\n",
        "\n",
        "#load the state dict into a newly instantiated instance of our model class.\n",
        "\n",
        "loaded_model_0 = LinearRegressionModel()\n",
        "\n",
        "loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "#will print with <All keys matched successfully>  ....\n",
        "\n",
        "loaded_model_0.state_dict()\n",
        "\n",
        "loaded_model_0.eval()\n",
        "with torch.inference_mode():\n",
        "  loaded_model_preds = loaded_model_0(x_test)\n",
        "\n",
        "loaded_model_preds\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "weight = 0.8\n",
        "bias = 0.2\n",
        "\n",
        "#Trying to build a model that can estimate the above values.\n",
        "\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "\n",
        "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
        "y = weight * X + bias\n",
        "\n",
        "#split data.\n",
        "\n",
        "train_split = int(0.8 * len(X))\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]\n",
        "\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)\n",
        "\n",
        "class LinearRegressionModelV2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    #Using nn.Linear() to create the model's parameters.\n",
        "    #called: linear transform or probing layer, dense layer etc.\n",
        "    self.linear_layer = nn.Linear(in_features = 1,\n",
        "                                  out_features = 1)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    return self.linear_layer(x)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model_1 = LinearRegressionModelV2()\n",
        "model_1, model_1.state_dict()\n",
        "\n",
        "#PyTorch has many built-in mathematical transofrmations.\n",
        "\n",
        "#Set the model to use the target device.\n",
        "\n",
        "next(model_1.parameters()).device\n",
        "\n",
        "\n",
        "model_1.to(device)\n",
        "next(model_1.parameters()).device\n",
        "\n",
        "###\n",
        "#Training code.\n",
        "\n",
        "#For training we need:\n",
        "#Loss function\n",
        "#Optimizer\n",
        "#Training loop\n",
        "#Testing loop\n",
        "#\n",
        "\n",
        "\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "optimizer = torch.optim.SGD(params= model_1.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "epochs = 200\n",
        "\n",
        "#Put data on the target device( DEVICE AGNOSTIC CODE FOR DATA.)\n",
        "#ALL OF THE THINGS YOU ARE COMPUTING WITH SHOULD BE ON THE SAME DEVICE.\n",
        "\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "x_test = x_test.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model_1.train()\n",
        "\n",
        "  #1.FORWARD PASS\n",
        "  y_pred = model_1(x_train)\n",
        "\n",
        "  #2. Calculate the loss\n",
        "  loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "  #3. Optimizer zero grad OTHERWISE THE OPTIMIZER WILL ACCUMULATE GRADIENTS.\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  #4. Perform backpropagation\n",
        "  loss.backward()\n",
        "\n",
        "  #5. Optimizer step\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing:\n",
        "  model_1.eval()\n",
        "  with torch.inference_mode():\n",
        "    test_pred = model_1(x_test)\n",
        "\n",
        "    test_loss = loss_fn(test_pred, y_test)\n",
        "\n",
        "  #Print out what's happening.\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}\")\n",
        "model_1.eval()\n",
        "with torch.inference_mode():\n",
        "  y_preds = model_1(x_test)\n",
        "y_preds\n",
        "\n",
        "plot_predictions(predictions=y_preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn #nn contains all of PyTorch's neural networks components.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_predictions(train_data=x_train,train_labels=y_train,test_data=x_test,\n",
        "                     test_labels=y_test,predictions=None):\n",
        "  plt.figure(figsize=(10,7))\n",
        "  plt.scatter(train_data,train_labels,c=\"b\",s=4,label=\"Training Data\")\n",
        "  plt.scatter(test_data,test_labels,c=\"g\",s=4,label=\"Testing Data\")\n",
        "  if predictions is not None:\n",
        "    plt.scatter(test_data,predictions,c=\"r\",s=4,label=\"Predictions\")\n",
        "  plt.legend(prop={\"size\":14});\n",
        "\n",
        "\n",
        "\n",
        "model_1.eval()\n",
        "with torch.inference_mode():\n",
        "  y_preds = model_1(x_test)\n",
        "y_preds\n",
        "\n",
        "#SAVING AND LOADING A TRAINED MODEL.\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "#Hyperparameters, epochs etc often written in all-caps\n",
        "\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "\n",
        "torch.save(obj=model_1.state_dict(),\n",
        "           f=MODEL_SAVE_PATH)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5qEZfHDBOcd",
        "outputId": "572d331a-4010-48e3-98de-c74cb9a15343"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to: models/01_bytorch_workflow_model_1.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_1 = LinearRegressionModelV2()\n",
        "\n",
        "loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "\n",
        "loaded_model_1.to(device) #device agnostic code\n",
        "\n",
        "next(loaded_model_1.parameters()).device\n",
        "\n",
        "loaded_model_1.eval()\n",
        "with torch.inference_mode():\n",
        "  loaded_model_1_preds = loaded_model_1(x_test)\n",
        "\n",
        "#ExtraCurriculars:\n",
        "#01 extracurriculars.\n",
        "#\"What is backpropagation really doing by 3blue1brown\"\n",
        "#gradient descent vids by robert kwi and 3b1b\n",
        "#what is torch.nn really?\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IDddtdzKmL2",
        "outputId": "afa6b760-0835-4513-e5ac-a274354688b8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\"What is a classification problem?\"\n",
        "#\"Spam or not spam\" (Binary classification)\n",
        "#imagenet dataset for computer vision\n",
        "#imagenet 1k\n",
        "#multi-label classification : \"Categories on a wikipedia page\"\n",
        "#multi-class classification : \"One of many different things\"\n",
        "# Architecture of a neural network classification model...\n",
        "#Input shapes and output shapes of a classification model (FEATURES AND LABELS)\n",
        "#Creating custom data to view fit on and predict on\n",
        "#Steps in modelling:\n",
        "  #Creating a model, setting a loss function and optimiser, creating a training\n",
        "  #loop, evaluating the model.\n",
        "#Saving and loading models\n",
        "#\"Harnessing the power of nonlinearity\"\n",
        "#Different Classification evaluation methods\n",
        "\n",
        "#Classification INPUTS and OUTPUTS.\n",
        "\n",
        "#224x224x3 WIDTH HEIGHT COLOR\n",
        "\n",
        "#Above a certain probability: generate a label.\n",
        "#Usually also batch_size.\n",
        "#Different orderings may occur.\n",
        "#Width and height are usually together.\n",
        "#Batch_size is often 32. \"minibatches\" batch_size, colour_channels, width, height\n",
        "#Looks at 32 images at a time.\n",
        "\n",
        "#Architecture of a classification model:\n",
        "#Input layer shape(in features) -> This is the same as the number of features.\n",
        "#eg. age, sex, height, weight, etc.\n",
        "#Minimum of one hidden layer.\n",
        "#Neurons per hidden layer, Usually min 10 or max 512.\n",
        "#Behind the scenes, PyTorch creates each node.\n",
        "#One output node per class being detected.\n",
        "#Hidden layer activation: ReLU is common; Rectified linear Unit.....\n",
        "#Output activation; Sigmoid.\n",
        "#loss function->depends on what type of classification task you are trying to do\n",
        "# CODE ----->\n",
        "\n",
        "#Neural network classification with PyTorch.\n",
        "#All code is also recorded under Github Repo.\n",
        "\n",
        "#\n",
        "\n"
      ],
      "metadata": {
        "id": "UO6YzZSoaCF8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}